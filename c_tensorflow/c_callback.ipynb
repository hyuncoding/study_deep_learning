{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510d1506-8773-43ba-8d23-ea3988fb935b",
   "metadata": {},
   "source": [
    "### Callback API\n",
    "\n",
    "- 모델이 학습 중에 충돌이 발생하거나 네트워크가 끊기면, 모든 훈련 시간이 낭비될 수 있고,  \n",
    "  과적합을 방지하기 위해 훈련을 중간에 중지해야 할 수도 있다.\n",
    "- 모델이 학습을 시작하면 학습이 완료될 때까지 아무런 제어를 하지 못하게 되고,  \n",
    "  신경망 훈련을 완료하는 데에는 몇 시간 또는 며칠이 걸릴 수 있기 때문에 모델을 모니터링하고 제어할 수 있는 기능이 필요하다.\n",
    "- 훈련 시(`fit()`) `Callback API`를 등록시키면 반복 내에서 특정 이벤트 발생마다 등록된 `callback`이 호출되어 수행된다.\n",
    "\n",
    "**ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weight_only=False, mode='auto')**\n",
    "\n",
    "- 특정 조건에 따라 모델 또는 가중치를 파일로 저장한다.\n",
    "- filepath: \"weights.{epoch:03d}-{val_loss:.4f}-{acc:.4f}.weights.hdf5\"와 같이 모델의 체크포인트를 저장한다.\n",
    "- monitor: 모니터링할 성능 지표를 작성한다.\n",
    "- save_best_only: 가장 좋은 성능을 나타내는 모델을 저장할지에 대한 여부\n",
    "- save_weights_only: weights만 저장할지에 대한 여부\n",
    "- mode: {auto, min, max} 중 한 가지를 작성한다. monitor의 성능 지표에 따라 좋은 경우를 선택한다.  \n",
    "  *monitor의 성능 지표가 감소해야 좋은 경우 min, 증가해야 좋은 경우 max, monitor의 이름으로부터 자동으로 유추하고 싶다면 auto\n",
    "\n",
    "**ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto',min_lr=0)**\n",
    "\n",
    "- 특정 반복동안 성능이 개선되지 않을 때, 학습률을 동적으로 감소시킨다.\n",
    "- monitor: 모니터링할 성능 지표를 작성한다.\n",
    "- factor: 학습률을 감소시킬 비유, 새로운 학습률 = 기존 학습률 * factor\n",
    "- patience: 학습률을 줄이기 전에 monitor할 반복 횟수\n",
    "- mode: {auto, min, max} 중 한 가지를 작성한다. monitor의 성능 지표에 따라 좋은 경우를 선택한다.  \n",
    "  *monitor의 성능 지표가 감소해야 좋은 경우 min, 증가해야 좋은 경우 max, monitor의 이름으로부터 자동으로 유추하고 싶다면 auto\n",
    "\n",
    "**EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')**\n",
    "\n",
    "- 특정 반복동안 성능이 개선되지 않을 때, 학습을 조기에 중단한다.\n",
    "- monitor: 모니터링할 성능 지표를 작성한다.\n",
    "- patience: Early Stopping을 적용하기 전에 monitor할 반복 횟수\n",
    "- mode: {auto, min, max} 중 한 가지를 작성한다. monitor의 성능 지표에 따라 좋은 경우를 선택한다.  \n",
    "  *monitor의 성능 지표가 감소해야 좋은 경우 min, 증가해야 좋은 경우 max, monitor의 이름으로부터 자동으로 유추하고 싶다면 auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6589f37d-43d2-4b24-8e9a-9f02486d0765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, Input, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "INPUT_SIZE = 28\n",
    "\n",
    "def create_model():\n",
    "    input_tensor = Input(shape=(INPUT_SIZE, INPUT_SIZE))\n",
    "    x = Flatten()(input_tensor)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    output = Dense(10, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_tensor, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2762a169-23c5-44e7-affd-406321c5c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_preprocessed_data(images, targets):\n",
    "    images = np.array(images / 255.0, dtype=np.float32)\n",
    "    targets = np.array(targets, dtype=np.float32)\n",
    "\n",
    "    return images, targets\n",
    "\n",
    "def get_preprocessed_ohe(images, targets):\n",
    "    images, targets = get_preprocessed_data(images, targets)\n",
    "    oh_targets = to_categorical(targets)\n",
    "\n",
    "    return images, oh_targets\n",
    "\n",
    "def get_train_valid_test(train_images, train_targets, test_images, test_targets, validation_size=0.2, random_state=124):\n",
    "    train_images, train_oh_targets = get_preprocessed_ohe(train_images, train_targets)\n",
    "    test_images, test_oh_targets = get_preprocessed_ohe(test_images, test_targets)\n",
    "\n",
    "    train_images, validation_images, train_oh_targets, validation_oh_targets = \\\n",
    "    train_test_split(train_images, train_oh_targets, stratify=train_oh_targets, test_size=validation_size, random_state=random_state)\n",
    "\n",
    "    return (train_images, train_oh_targets), (validation_images, validation_oh_targets), (test_images, test_oh_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0eb7846-26d2-47b0-9a9e-dd8b2a1e5d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 28, 28) (48000, 10)\n",
      "(12000, 28, 28) (12000, 10)\n",
      "(10000, 28, 28) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "(train_images, train_targets), (test_images, test_targets) = fashion_mnist.load_data()\n",
    "\n",
    "(train_images, train_oh_targets), (validation_images, validation_oh_targets), (test_images, test_oh_targets) = \\\n",
    "get_train_valid_test(train_images, train_targets, test_images, test_targets)\n",
    "\n",
    "print(train_images.shape, train_oh_targets.shape)\n",
    "print(validation_images.shape, validation_oh_targets.shape)\n",
    "print(test_images.shape, test_oh_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b3feac6-f874-4e85-8247-a00ff35fb0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C 드라이브의 볼륨: OS\n",
      " 볼륨 일련 번호: E0EA-245E\n",
      "\n",
      " C:\\kdt_0900_yh\\ai\\deep_learning\\c_tensorflow\\callback_files 디렉터리\n",
      "\n",
      "2024-05-28  오전 10:16    <DIR>          .\n",
      "2024-05-28  오전 10:14    <DIR>          ..\n",
      "2024-05-28  오전 10:16           739,600 weights.001-0.4115-0.8069.weights.h5\n",
      "2024-05-28  오전 10:14           739,600 weights.001-0.5045-0.8086.weights.h5\n",
      "2024-05-28  오전 10:14           739,600 weights.002-0.3598-0.8583.weights.h5\n",
      "2024-05-28  오전 10:16           739,600 weights.002-0.3729-0.8567.weights.h5\n",
      "2024-05-28  오전 10:14           739,600 weights.003-0.3594-0.8733.weights.h5\n",
      "2024-05-28  오전 10:16           739,600 weights.003-0.3669-0.8699.weights.h5\n",
      "2024-05-28  오전 10:14           739,600 weights.004-0.3356-0.8783.weights.h5\n",
      "2024-05-28  오전 10:16           739,600 weights.004-0.3461-0.8791.weights.h5\n",
      "2024-05-28  오전 10:16           739,600 weights.005-0.3483-0.8857.weights.h5\n",
      "2024-05-28  오전 10:14           739,600 weights.006-0.3242-0.8899.weights.h5\n",
      "2024-05-28  오전 10:16           739,600 weights.006-0.3302-0.8876.weights.h5\n",
      "2024-05-28  오전 10:16           739,600 weights.007-0.3167-0.8946.weights.h5\n",
      "2024-05-28  오전 10:14           739,600 weights.008-0.3112-0.9000.weights.h5\n",
      "2024-05-28  오전 10:16           739,600 weights.008-0.3301-0.8999.weights.h5\n",
      "2024-05-28  오전 10:16           739,600 weights.009-0.3119-0.9016.weights.h5\n",
      "2024-05-28  오전 10:16           739,600 weights.010-0.3305-0.9062.weights.h5\n",
      "2024-05-28  오전 10:16           739,600 weights.011-0.3213-0.9079.weights.h5\n",
      "2024-05-28  오전 10:16           739,600 weights.012-0.3484-0.9093.weights.h5\n",
      "2024-05-28  오전 10:16           739,600 weights.013-0.3031-0.9130.weights.h5\n",
      "2024-05-28  오전 10:14           739,600 weights.013-0.3052-0.9137.weights.h5\n",
      "2024-05-28  오전 10:16           739,600 weights.014-0.3496-0.9158.weights.h5\n",
      "2024-05-28  오전 10:16           739,600 weights.015-0.3171-0.9181.weights.h5\n",
      "2024-05-28  오전 10:16           739,600 weights.016-0.3315-0.9207.weights.h5\n",
      "2024-05-28  오전 10:16           739,600 weights.017-0.3215-0.9233.weights.h5\n",
      "2024-05-28  오전 10:16           739,600 weights.018-0.3461-0.9248.weights.h5\n",
      "2024-05-28  오전 10:16           739,600 weights.019-0.3355-0.9249.weights.h5\n",
      "2024-05-28  오전 10:16           739,600 weights.020-0.3249-0.9290.weights.h5\n",
      "              27개 파일          19,969,200 바이트\n",
      "               2개 디렉터리  548,187,500,544 바이트 남음\n"
     ]
    }
   ],
   "source": [
    " !dir callback_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2192c892-d413-42b5-aca4-d384d0928dd3",
   "metadata": {},
   "source": [
    "### ModelCheckPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c12dbcbb-d317-48de-ada9-221c387d19c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.7399 - loss: 0.7578 - val_acc: 0.8484 - val_loss: 0.4188\n",
      "Epoch 2/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.8516 - loss: 0.4171 - val_acc: 0.8585 - val_loss: 0.3966\n",
      "Epoch 3/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8684 - loss: 0.3675 - val_acc: 0.8673 - val_loss: 0.3632\n",
      "Epoch 4/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8772 - loss: 0.3346 - val_acc: 0.8777 - val_loss: 0.3338\n",
      "Epoch 5/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8859 - loss: 0.3144 - val_acc: 0.8750 - val_loss: 0.3372\n",
      "Epoch 6/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8885 - loss: 0.2995 - val_acc: 0.8758 - val_loss: 0.3304\n",
      "Epoch 7/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8950 - loss: 0.2851 - val_acc: 0.8748 - val_loss: 0.3406\n",
      "Epoch 8/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8990 - loss: 0.2714 - val_acc: 0.8865 - val_loss: 0.3097\n",
      "Epoch 9/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9026 - loss: 0.2633 - val_acc: 0.8825 - val_loss: 0.3202\n",
      "Epoch 10/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9071 - loss: 0.2489 - val_acc: 0.8867 - val_loss: 0.3127\n",
      "Epoch 11/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9101 - loss: 0.2418 - val_acc: 0.8722 - val_loss: 0.3432\n",
      "Epoch 12/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9129 - loss: 0.2343 - val_acc: 0.8828 - val_loss: 0.3306\n",
      "Epoch 13/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9125 - loss: 0.2321 - val_acc: 0.8901 - val_loss: 0.3135\n",
      "Epoch 14/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9182 - loss: 0.2192 - val_acc: 0.8779 - val_loss: 0.3430\n",
      "Epoch 15/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9198 - loss: 0.2137 - val_acc: 0.8843 - val_loss: 0.3331\n",
      "Epoch 16/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9195 - loss: 0.2132 - val_acc: 0.8911 - val_loss: 0.3159\n",
      "Epoch 17/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9247 - loss: 0.2035 - val_acc: 0.8932 - val_loss: 0.3204\n",
      "Epoch 18/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9268 - loss: 0.1947 - val_acc: 0.8801 - val_loss: 0.3488\n",
      "Epoch 19/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9299 - loss: 0.1840 - val_acc: 0.8913 - val_loss: 0.3209\n",
      "Epoch 20/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9326 - loss: 0.1830 - val_acc: 0.8884 - val_loss: 0.3389\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = create_model()\n",
    "model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['acc'])\n",
    "\n",
    "# mcp_cb = ModelCheckpoint(\n",
    "#     filepath=\"./callback_files/weights.{epoch:03d}-{val_loss:.4f}-{acc:.4f}.weights.h5\",\n",
    "#     monitor='val_loss',\n",
    "#     # 모든 epoch의 파일을 저장하지 않고 좋은 성능이라 판단될 경우만 저장할 때 True 설정\n",
    "#     save_best_only=False,\n",
    "#     save_weights_only=True,\n",
    "#     mode='min'\n",
    "# )\n",
    "\n",
    "mcp_cb = ModelCheckpoint(\n",
    "    filepath=\"./callback_files/model.{epoch:03d}-{val_loss:.4f}-{acc:.4f}.model.keras\",\n",
    "    monitor='val_loss',\n",
    "    # 모든 epoch의 파일을 저장하지 않고 좋은 성능이라 판단될 경우만 저장할 때 True 설정\n",
    "    save_best_only=False,\n",
    "    save_weights_only=False,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "history = model.fit(x=train_images, y=train_oh_targets, \n",
    "                    validation_data=(validation_images, validation_oh_targets), \n",
    "                    epochs=20, batch_size=64, callbacks=[mcp_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b75db33e-099a-4b65-8494-cd576885fc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - acc: 0.8798 - loss: 0.3792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.37800469994544983, 0.8809999823570251]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images, test_oh_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03a307bc-60a1-432b-829c-91045a0d6806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - acc: 0.8815 - loss: 0.3673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3669484853744507, 0.8791000247001648]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.load_weights('./callback_files/weights.020-0.3362-0.9317.weights.h5')\n",
    "\n",
    "model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['acc'])\n",
    "\n",
    "model.evaluate(test_images, test_oh_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87760e14-cae1-43d7-8fb8-e3adcf497e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 808us/step - acc: 0.8798 - loss: 0.3792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.37800469994544983, 0.8809999823570251]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('./callback_files/model.020-0.3389-0.9292.model.keras')\n",
    "model.evaluate(test_images, test_oh_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873e4e9c-d878-4286-9c21-277a7dd9204d",
   "metadata": {},
   "source": [
    "### ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81d4ca6d-4d46-421a-b4fb-e7f79736e2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.7442 - loss: 0.7533 - val_acc: 0.8518 - val_loss: 0.4110 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8536 - loss: 0.4116 - val_acc: 0.8553 - val_loss: 0.3982 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8679 - loss: 0.3604 - val_acc: 0.8641 - val_loss: 0.3622 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8782 - loss: 0.3363 - val_acc: 0.8684 - val_loss: 0.3516 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8874 - loss: 0.3073 - val_acc: 0.8730 - val_loss: 0.3385 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.8939 - loss: 0.2908 - val_acc: 0.8748 - val_loss: 0.3374 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8968 - loss: 0.2807 - val_acc: 0.8834 - val_loss: 0.3185 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9021 - loss: 0.2693 - val_acc: 0.8766 - val_loss: 0.3338 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9054 - loss: 0.2571 - val_acc: 0.8859 - val_loss: 0.3189 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9176 - loss: 0.2224 - val_acc: 0.8911 - val_loss: 0.2994 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9220 - loss: 0.2111 - val_acc: 0.8921 - val_loss: 0.2984 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9223 - loss: 0.2136 - val_acc: 0.8908 - val_loss: 0.2975 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9235 - loss: 0.2101 - val_acc: 0.8923 - val_loss: 0.2996 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9257 - loss: 0.2023 - val_acc: 0.8927 - val_loss: 0.2954 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9278 - loss: 0.2003 - val_acc: 0.8951 - val_loss: 0.2962 - learning_rate: 1.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9304 - loss: 0.1945 - val_acc: 0.8927 - val_loss: 0.2969 - learning_rate: 1.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9313 - loss: 0.1890 - val_acc: 0.8946 - val_loss: 0.2940 - learning_rate: 1.0000e-05\n",
      "Epoch 18/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9295 - loss: 0.1928 - val_acc: 0.8948 - val_loss: 0.2940 - learning_rate: 1.0000e-05\n",
      "Epoch 19/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9308 - loss: 0.1912 - val_acc: 0.8947 - val_loss: 0.2936 - learning_rate: 1.0000e-05\n",
      "Epoch 20/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9296 - loss: 0.1943 - val_acc: 0.8953 - val_loss: 0.2938 - learning_rate: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "model = create_model()\n",
    "model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['acc'])\n",
    "\n",
    "rlr_cb = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "history = model.fit(x=train_images, y=train_oh_targets, \n",
    "                    validation_data=(validation_images, validation_oh_targets), \n",
    "                    epochs=20, batch_size=64, callbacks=[rlr_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a49b026-0dc6-4c82-b0ad-a4f105e98c56",
   "metadata": {},
   "source": [
    "### EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fc205f4-7144-4651-a7b0-70843543cb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - acc: 0.7497 - loss: 0.7252 - val_acc: 0.8397 - val_loss: 0.4318\n",
      "Epoch 2/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8538 - loss: 0.4077 - val_acc: 0.8650 - val_loss: 0.3707\n",
      "Epoch 3/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8713 - loss: 0.3552 - val_acc: 0.8646 - val_loss: 0.3772\n",
      "Epoch 4/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.8776 - loss: 0.3365 - val_acc: 0.8725 - val_loss: 0.3499\n",
      "Epoch 5/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8842 - loss: 0.3120 - val_acc: 0.8777 - val_loss: 0.3383\n",
      "Epoch 6/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.8914 - loss: 0.2975 - val_acc: 0.8702 - val_loss: 0.3524\n",
      "Epoch 7/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8971 - loss: 0.2756 - val_acc: 0.8808 - val_loss: 0.3245\n",
      "Epoch 8/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8997 - loss: 0.2708 - val_acc: 0.8799 - val_loss: 0.3259\n",
      "Epoch 9/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9034 - loss: 0.2617 - val_acc: 0.8795 - val_loss: 0.3359\n",
      "Epoch 10/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9021 - loss: 0.2602 - val_acc: 0.8691 - val_loss: 0.3472\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = create_model()\n",
    "model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['acc'])\n",
    "\n",
    "ely_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "history = model.fit(x=train_images, y=train_oh_targets, \n",
    "                    validation_data=(validation_images, validation_oh_targets), \n",
    "                    epochs=20, batch_size=64, callbacks=[ely_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6bb14ba-e430-410d-83cc-5e3aa2e073ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - acc: 0.7378 - loss: 0.7558 - val_acc: 0.8467 - val_loss: 0.4243 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.8535 - loss: 0.4138 - val_acc: 0.8641 - val_loss: 0.3746 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.8690 - loss: 0.3632 - val_acc: 0.8663 - val_loss: 0.3641 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.8755 - loss: 0.3423 - val_acc: 0.8763 - val_loss: 0.3361 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8885 - loss: 0.3088 - val_acc: 0.8734 - val_loss: 0.3484 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.8884 - loss: 0.2988 - val_acc: 0.8668 - val_loss: 0.3665 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9050 - loss: 0.2626 - val_acc: 0.8842 - val_loss: 0.3109 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9082 - loss: 0.2518 - val_acc: 0.8848 - val_loss: 0.3067 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9099 - loss: 0.2459 - val_acc: 0.8874 - val_loss: 0.3063 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9102 - loss: 0.2429 - val_acc: 0.8866 - val_loss: 0.3065 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9143 - loss: 0.2364 - val_acc: 0.8878 - val_loss: 0.3053 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9120 - loss: 0.2387 - val_acc: 0.8882 - val_loss: 0.3071 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9137 - loss: 0.2357 - val_acc: 0.8893 - val_loss: 0.3024 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9131 - loss: 0.2385 - val_acc: 0.8902 - val_loss: 0.3033 - learning_rate: 1.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9151 - loss: 0.2313 - val_acc: 0.8893 - val_loss: 0.3024 - learning_rate: 1.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - acc: 0.9165 - loss: 0.2258 - val_acc: 0.8900 - val_loss: 0.3002 - learning_rate: 1.0000e-05\n",
      "Epoch 17/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9168 - loss: 0.2266 - val_acc: 0.8901 - val_loss: 0.3002 - learning_rate: 1.0000e-05\n",
      "Epoch 18/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9169 - loss: 0.2260 - val_acc: 0.8898 - val_loss: 0.3002 - learning_rate: 1.0000e-05\n",
      "Epoch 19/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9161 - loss: 0.2272 - val_acc: 0.8898 - val_loss: 0.3001 - learning_rate: 1.0000e-06\n",
      "Epoch 20/20\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9180 - loss: 0.2251 - val_acc: 0.8898 - val_loss: 0.3000 - learning_rate: 1.0000e-06\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = create_model()\n",
    "model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['acc'])\n",
    "\n",
    "mcp_cb = ModelCheckpoint(\n",
    "    filepath=\"./callback_files/weights.{epoch:03d}-{val_loss:.4f}-{acc:.4f}.weights.h5\",\n",
    "    monitor='val_loss',\n",
    "    # 모든 epoch의 파일을 저장하지 않고 좋은 성능이라 판단될 경우만 저장할 때 True 설정\n",
    "    save_best_only=False,\n",
    "    save_weights_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "rlr_cb = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "ely_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "history = model.fit(x=train_images, y=train_oh_targets, \n",
    "                    validation_data=(validation_images, validation_oh_targets), \n",
    "                    epochs=20, batch_size=64, callbacks=[mcp_cb, rlr_cb, ely_cb])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
